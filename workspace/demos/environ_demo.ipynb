{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Demo\n",
    "\n",
    "This notebook demonstrates key features of the `hds-sparkhive-lab` environment:\n",
    "- Creating and using databases\n",
    "- Creating Delta tables with Hive metastore\n",
    "- Querying tables using `database.table` syntax\n",
    "- Updating Delta tables\n",
    "- Time travel queries\n",
    "- Hive metastore metadata commands\n",
    "\n",
    "Run all cells step-by-step to explore the environment.\n",
    "\n",
    "**Notes:**\n",
    "- Use `database.table` syntax to reference tables, ensuring Hive metastore metadata is used.\n",
    "- This environment mimics Databricks behavior with Delta Lake and Hive Metastore.\n",
    "- Data and metadata are persisted via Docker volumes.\n",
    "- You can connect to this kernel from VS Code or use Jupyter Lab UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required PySpark modules\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"hds_sparkhive_demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set current database\n",
    "spark.catalog.setCurrentDatabase(\"demo_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
    "columns = [\"id\", \"name\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame as Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"demo_db.people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the Delta table using database.table syntax\n",
    "spark.sql(\"SELECT * FROM demo_db.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|    default|\n",
      "|    demo_db|\n",
      "|my_database|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show all databases\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|  demo_db|   people|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show tables in the current database\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|   bigint|   NULL|\n",
      "|    name|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table schema\n",
    "spark.sql(\"DESCRIBE demo_db.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a record in the Delta table\n",
    "delta_table = DeltaTable.forName(spark, \"demo_db.people\")\n",
    "delta_table.update(\"id = 2\", {\"name\": \"'Bobby'\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|  Bobby|\n",
      "|  3|Charlie|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query again to see the update\n",
    "spark.sql(\"SELECT * FROM demo_db.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6b326bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2025-07-28 23:00:...|  NULL|    NULL|              UPDATE|{predicate -> [\"(...|NULL|    NULL|     NULL|          1|  Serializable|        false|{numRemovedFiles ...|        NULL|Apache-Spark/3.5....|\n",
      "|      1|2025-07-28 22:58:...|  NULL|    NULL|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "|      0|2025-07-28 22:48:...|  NULL|    NULL|CREATE OR REPLACE...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 1, n...|        NULL|Apache-Spark/3.5....|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the Delta table history\n",
    "spark.sql(\"DESCRIBE HISTORY demo_db.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time travel: query previous version\n",
    "spark.sql(\"SELECT * FROM demo_db.people VERSION AS OF 0\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
